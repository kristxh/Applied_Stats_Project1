---
title: "6372 Project 1 - Life Expectancy"
author: "By Kristi Herman, Bo Yun, Eric Romero, Neil Benson"
date: "09/20/2020"
output:
  html_document:
    df_print: paged
---
  
# Introduction
Introduction paragraphs here


## Importing the necessary libraries
```{r,warning=FALSE,message=FALSE}

library(ggplot2)
library(ggthemes)
library(gridExtra)
library(tidyverse)
library(plyr)
library(dplyr) 
library(naniar) #keep
library(zoo)


# create not in operator
`%notin%` <- Negate(`%in%`)

```
## Introduction
What factors impact life expectancy?  Can life expectancy be predicted?  The World Health Organization (WHO) maintains a database with life expectancy information for all countries by year along with other variables that could contribute to life expectancy.  The data falls into four main areas:  health, economic, social, and immunization.  This project makes use of the WHO data to better understand life expectancy and how to predict it.  In this paper, three different models will be explored:
1)      A regression model to identify key relationships between life expectancy and factors related to health, economic, social, and immunization.
2)      A parametric regression model to predict life expectancy.
3)      A non-parametric regression model to predict life expectancy.


## Data Description
The Life Expectancy dataset used in this analysis was collected from the WHO and made available at Kaggle:  https://www.kaggle.com/kumarajarshi/life-expectancy-who.  It includes data from 193 countries for the years 2000 – 2015.  The 2938 rows contained in the dataset include average life expectancy per country for each year.  In addition to the average life expectancy, there are 21 columns of data related to health, economic, social, and immunization.   


## Exploratory Data Analysis
  
## Importing and Cleaning the Data
```{r, warning=FALSE, message=FALSE}
########### Loading the data
life_ex_DF <- read.csv("https://raw.githubusercontent.com/kristxh/Applied_Stats_Project1/master/datasets_12603_17232_Life_Expectancy_Data.csv")

# cleaning up column names
colnames(life_ex_DF) <- tolower(colnames(life_ex_DF))
colnames(life_ex_DF) <- gsub("\\.\\.",".",colnames(life_ex_DF))
colnames(life_ex_DF) <- gsub("\\.","_",colnames(life_ex_DF))
names(life_ex_DF)[19] <- "thinness_10_19_years"

# converting status and year to factor columns
life_ex_DF$status <- as.factor(life_ex_DF$status)
```


## Checking for missing data
```{r, fig.height=7, fig.width=9}
# checking for missing values in the data
vis_miss(life_ex_DF) + xlab("Data Columns")
```
  
  
## Fix missing data
```{r, fig.height=7, fig.width=9}
# sorting the columns so that life_expectancy column is first
life_ex_DF <- life_ex_DF %>% select("life_expectancy", everything())

factorcols <- c("status","country","year")
allcols <- colnames(life_ex_DF)

# selecting only numerical columns for correlation
corvars <- allcols[allcols %notin% factorcols]


# impute with median missing values per country
life_ex_median_DF <- as.data.frame(life_ex_DF %>%
  group_by(country) %>%
  mutate_at(corvars, na.aggregate, FUN=median))


# dropping rows where the country has a single entry/record in the dataset and their records are mostly incomplete
life_ex_median_DF <- life_ex_median_DF[!is.na(life_ex_median_DF$life_expectancy), ]


# imputing the median by country still left a few nulls throughout. We will then impute the rest of the missing values as the mean of status (developing, developed, by year)
life_ex_cleaned_DF <- as.data.frame(life_ex_median_DF %>%
  group_by(status,year) %>%
  mutate_at(corvars, na.aggregate, FUN=median))

# checking for missing values in the data
vis_miss(life_ex_cleaned_DF) + xlab("Data Columns")
```


## Summary of cleaned up data
```{r}
summary(life_ex_cleaned_DF)
str(life_ex_cleaned_DF)

t(aggregate(life_expectancy~status,data=life_ex_cleaned_DF,summary))
t(aggregate(life_expectancy~status,data=life_ex_cleaned_DF,sd))
```
  
BMI has some very weird mins and maxes. Will explore further, but this data may later show correlations that aren't actually there because the min and max of BMI are not practical or possible.  


## Histogram of life_expectancy
```{r, fig.height=7, fig.width=9}
theme_set(theme_igray())

le_hist <- geom_histogram(
  mapping = aes(life_expectancy),
  data = life_ex_cleaned_DF,
  bins = 20,
  colour = "gray",
  fill = "#0072B2"
)

ggplot()+ le_hist + xlab("Years") + ggtitle("Life Expectancy")
```


## Overall Life Expectancy
```{r, fig.height=7, fig.width=9}

le_by_yr <- ddply(life_ex_cleaned_DF, .(year), summarize,  avg_le=mean(life_expectancy))

le_line <- geom_line(
  mapping = aes(x = year, y = avg_le),
  data = le_by_yr,
  size = 1,
  colour = "dodgerblue4"
)

ggplot() + le_line + xlab("Year") + ylab("Life Expectancy") + ggtitle("Average Life Expectancy: 2000 - 2015") 

```
Life Expectancy is linearly related to year, and year should be treated as continuous instead of as a factor in this model. We will drop BMI for having unrealistic range of data, and no way to correct for it.


## Developed/Developing Life Expectancy
```{r, fig.height=7, fig.width=9}
le_by_status <- ddply(life_ex_cleaned_DF, .(year,status), summarize,  avg_le=mean(life_expectancy))
line_colors <- c("dodgerblue4", "#D55E00")

le_line2 <- geom_line(
  mapping = aes(x = year, y = avg_le, group=status, color=status),
  data = le_by_status,
  size = 1
)

ggplot() + le_line2 +  xlab("Year") + ylab("Life Expectancy") + ggtitle("Average Life Expectancy: Developed and Developing Countries") + scale_colour_manual(values=line_colors)

```


## Boxplot by status (developing, developed)
```{r, fig.height=7, fig.width=9}
# Plot boxplot

dev_box <- geom_boxplot(
  mapping = aes(x=status, y=life_expectancy),
  data = life_ex_cleaned_DF,
  colour = "black",
  fill = "#56B4E9",
  groupColors=c('#999999','#E69F00')
)

ggplot()+ dev_box + xlab("Status of Country") + ylab("Life Expectancy") + ggtitle("Life Expectancy: Developed and Developing Countries ")

```
There appears to be enough of a difference between `life_expectancy` mean by developed and developing `status`. We will perform an anova and analyze signifance in the difference in means. 


## Boxplots by status by year
```{r, fig.height=7, fig.width=9}
# Plot boxplot

yr_box <- stat_boxplot(
  mapping = aes(x=year, y=life_expectancy, group=year),
  data = life_ex_cleaned_DF,
  colour = "black",
  fill = "#56B4E9"
  ) 
  

ggplot()+ yr_box + xlab("Year") + ylab("Life Expectancy") + scale_x_continuous(breaks=c(2000:2015)) + ggtitle("Life Expectancy by Year")  
```


## Checking the significance of the factorial variables using ANOVA
```{r}
# convert categorical variables to factors
names <- c("country", "status")

life_ex_cleaned_factor_DF <- life_ex_cleaned_DF

# set the categorical columns to factor
for (name in names)
{
  life_ex_cleaned_factor_DF[,name]<-factor(life_ex_cleaned_DF[,name])
}

# convert factors to numeric for factor analysis
life_ex_cleaned_asnum_DF <- life_ex_cleaned_factor_DF[,names] %>% mutate_all(as.numeric)

# adding suffix ASNUM to numerical representation of factors columns
colnames(life_ex_cleaned_asnum_DF) <- paste(colnames(life_ex_cleaned_asnum_DF), "ASNUM", sep = "_")

# adding numerical factor columns to base Df in order to perform stepwise for variable selection
life_ex_cleaned_withnum_DF <- cbind(life_ex_cleaned_factor_DF, life_ex_cleaned_asnum_DF)

# dropping factor columns
life_ex_cleaned_withnum_DF <- life_ex_cleaned_withnum_DF[, sapply(life_ex_cleaned_withnum_DF, class) != "factor"]

#  columns for anova factor analysis
anovacols <- c(colnames(life_ex_cleaned_asnum_DF))

#  creating the formula to pass into aov function
anovafmla <- as.formula(paste("life_expectancy ~ ", paste(anovacols, collapse= "+")))

# passing my auto generated formula to analyze
anova_fit <- aov(anovafmla, data=life_ex_cleaned_withnum_DF)
summary(anova_fit)
```


### Testing the life_expectancy mean difference by status  
Equal means model significance here to test significance of mean difference of `life_expectancy` between status.  


H0: μdeveloped = μdeveloping  
Ha: μdeveloped ≠ μdeveloping  

Reject H0 that the means are equal  

This is statistically significant (p-value <2e-16), and we will include status as a factor to consider in the model.  


### Both year and status are statistically significant as categorical predictors. 
Later will remove `country` from the data set for variable selection (p-value 0.367).


## Scatterplot boxplot
```{r, fig.height=7, fig.width=9}
le_scatter <- geom_point(
  mapping = aes(x=year, y=life_expectancy, color=status),
  data = life_ex_cleaned_DF
  ) 
  

ggplot()+ le_scatter + xlab("Year") + ylab("Life Expectancy") + scale_x_continuous(breaks=c(2000:2015)) + ggtitle("Life Expectancy: Developed and Developing Countries")  + scale_colour_manual(values=line_colors)
```


## Correlation matrix with pearson correlation coefficients
```{r, fig.height=9, fig.width=9}
library(reshape2)

corr <- round(cor(life_ex_cleaned_DF[,corvars]),1)
melted_cormat <- melt(corr)


# Get lower triangle of the correlation matrix
  get_lower_tri<-function(cormat){
    cormat[lower.tri(cormat)] <- NA
    return(cormat)
  }
  

# reorder the correlation matrix
reorder_cormat <- function(cormat){
# Use correlation between variables as distance
dd <- as.dist((1-cormat)/2)
hc <- hclust(dd)
cormat <-cormat[hc$order, hc$order]
}


# reorder the correlation matrix
corr <- reorder_cormat(corr)

# get lower half
lower_half <- get_lower_tri(corr)

# reshape the correlation matrix
melted_cormat <- melt(lower_half, na.rm = TRUE)

# plot it
ggcorrmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
 scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
  midpoint = 0, limit = c(-1,1), space = "Lab", 
    name="Pearson\nCorrelation") +
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 8, hjust = 1), axis.title.x = element_blank(), axis.title.y = element_blank())+
 coord_fixed()


ggcorrmap + geom_text(aes(Var2, Var1, label = value), color = "black", size = 2.5)
```
It appears there is a high degree of correlation between the following:  
- `thinnes_5_9_years` & `thinnes_10_19_years`: 0.9  
- `percentage_expenditure` & `gdp`: 0.9  
- `under_5_deaths` & `infant_deaths`: 1.0  
- `schooling` & `income_composition_of_resources`: 0.8  
- `population` has 0 correlation with the response variable and will be removed as a predictor and is incomplete throughout the data set  


## Further examining the issue of multicolinearity
```{r, fig.height=9, fig.width=9}
pairs(life_ex_cleaned_DF[,corvars], pch = 19,  cex = 0.5,
      col = line_colors[life_ex_cleaned_DF$status],
      upper.panel=NULL)

```


## Further exploration of hiv_ads and percentage_expenditure
```{r, fig.height=7, fig.width=9}
par(mfrow=c(1,2))

plot(life_ex_cleaned_DF$hiv_aids,life_ex_cleaned_DF$life_expectancy, xlab="hiv_aids",ylab="life_expectancy")


plot(life_ex_cleaned_DF$percentage_expenditure,life_ex_cleaned_DF$life_expectancy, xlab="percentage_expenditure",ylab="life_expectancy")
```
It appears that both `hiv_aids` and `percentage_expenditure` have nonlinear relationships with the response variable. We will further investigate.  

There also appears to be some interaction between `status` and `percentage_expenditure`, and `status` and `schooling`. Further exploration needed.


```{r, fig.height=7, fig.width=9}

life_expectancy <- life_ex_cleaned_DF$life_expectancy
hiv_aids <- life_ex_cleaned_DF$hiv_aids

hiv_aids.model <- lm(life_expectancy~hiv_aids)

# visually inspecting the reltationship between life_expectancy and hiv_aids before adding complexity
par(mfrow=c(1,3))
plot(hiv_aids,life_expectancy, xlab="hiv_aids",ylab="life_expectancy")
new<-data.frame(hiv_aids=seq(0,50,.1))
lines(seq(0,50,.1),predict(hiv_aids.model,newdata=new),col="red",lwd=4)
plot(hiv_aids.model$fitted.values,hiv_aids.model$residuals,xlab="Fitted Values",ylab="Residuals")
plot(life_ex_cleaned_DF$hiv_aids,hiv_aids.model$residuals,xlab="hiv_aids",ylab="Residuals")


# adding complexity to relationship between life_expectancy and hiv_aids
hiv_aids.model2 <- lm(life_expectancy~hiv_aids+I(log(hiv_aids)))


# visually inspecting the reltationship between life_expectancy and hiv_aids after transformations
par(mfrow=c(1,3))
plot(hiv_aids,life_expectancy, xlab="hiv_aids",ylab="life_expectancy")
new<-data.frame(hiv_aids=seq(0,50,.1))
lines(seq(0,50,.1),predict(hiv_aids.model2,newdata=new),col="red",lwd=4)
plot(hiv_aids.model2$fitted.values,hiv_aids.model2$residuals,xlab="Fitted Values",ylab="Residuals")
plot(life_ex_cleaned_DF$hiv_aids,hiv_aids.model2$residuals,xlab="hiv_aids",ylab="Residuals")

```


```{r, fig.height=7, fig.width=9}

percentage_expenditure <- life_ex_cleaned_DF$percentage_expenditure

perc_exp.model <- lm(life_expectancy~percentage_expenditure)

# visually inspecting the reltationship between life_expectancy and percentage_expenditure before adding complexity
par(mfrow=c(1,3))
plot(percentage_expenditure,life_expectancy, xlab="percentage_expenditure",ylab="life_expectancy")
new2<-data.frame(percentage_expenditure=seq(0,20000,100))
lines(seq(0,20000,100),predict(perc_exp.model,newdata=new2),col="red",lwd=4)
plot(perc_exp.model$fitted.values,perc_exp.model$residuals,xlab="Fitted Values",ylab="Residuals")
plot(life_ex_cleaned_DF$hiv_aids,perc_exp.model$residuals,xlab="hiv_aids",ylab="Residuals")


# adding complexity to relationship between life_expectancy and percentage_expenditure
perc_exp.model2 <- lm(life_expectancy~percentage_expenditure+I(sqrt(percentage_expenditure)))


# visually inspecting the reltationship between life_expectancy and percentage_expenditure after transformations
par(mfrow=c(1,3))
plot(percentage_expenditure,life_expectancy, xlab="percentage_expenditure",ylab="life_expectancy")
new2<-data.frame(percentage_expenditure=seq(0,20000,100))
lines(seq(0,20000,100),predict(perc_exp.model2,newdata=new2),col="red",lwd=4)
plot(perc_exp.model2$fitted.values,perc_exp.model2$residuals,xlab="Fitted Values",ylab="Residuals")
plot(life_ex_cleaned_DF$percentage_expenditure,perc_exp.model2$residuals,xlab="percentage_expenditure",ylab="Residuals")
```


## Checking colinearity with VIF and removing redundant columns
```{r}
library(car)

full.model <- lm(life_expectancy~. ,data=life_ex_cleaned_DF[,corvars])

vif(full.model, data = life_ex_cleaned_DF)

# removing columns and re-evaluating
colstodelete <- c("under_five_deaths", "thinness_5_9_years", "gdp", "population", "bmi")

full.model1 <- lm(life_expectancy~. ,data=life_ex_cleaned_DF[,corvars] %>% select(-all_of(colstodelete)))

vif(full.model1, data = life_ex_cleaned_DF[,corvars] %>% select(-all_of(colstodelete)))

```
- `infant_deaths` and `under_five_deaths` have a VIF of 177.36 and 176.35 respectively, indicating a very high degree of multicolinearity between the two. We will remove `under_five_deaths` and re-evaluate.  
  
- `thinness_5_9_years` and `thinness_10_19_years` have a VIF of 8.82 and 8.74 respectively, indicating a very high degree of multicolinearity between the two. We will remove `thinness_5_9_years` and re-evaluate.  
  
- `percentage_expenditure` and `gdp` have a VIF of 4.92 and 5.32 respectively, indicating a very high degree of multicolinearity between the two. We will remove `gdp` and re-evaluate.  
  
- `schooling` and `income_composition_of_resources` are correlated to each other with a high Pearson Correlation Coefficient of .8, but have VIFs of 3.62 and 3.05 respectively. While these are highly correlated, their VIFs are relatively low. We will leave them in them in for variable selection.  
  

## Removing columns with high colinearity or low significance
"under_five_deaths", "thinness_5_9_years", "gdp", "population" and "country"
Removing "bmi" as a result of unrealistic data
```{r}
# adding "country" to the list of columns to delete after its poor performance in ANOVA analysis
colstodelete <- append(colstodelete, "country")  
Life <- life_ex_cleaned_factor_DF %>% select(-all_of(colstodelete)) 
```


## Checking residuals and q-q plot of the full model
Including complexity for the terms `hiv_aids` and `percentage_expenditure`.  
```{r, fig.height=7, fig.width=9}
full.model <- lm(life_expectancy~. + I(hiv_aids^2) + I(sqrt(percentage_expenditure)) ,data=Life)
par(mfrow=c(2,2))
plot(full.model)
```
#####################
ADD COMMENTARY HERE ABOUT CHECKING THE RESIDUALS OF THE MODEL
#####################

## Adding interaction and checking significance
Visual assessment of pair plots showed potential interactions between the following:  
- `status` >> `percentage_expenditure`  
- `status` >> `schooling`  


```{r, fig.height=7, fig.width=9}
full.modelwinter <- lm(life_expectancy~. + I(hiv_aids^2) + I(sqrt(percentage_expenditure)) + status:percentage_expenditure + status:schooling, data=Life)

# Re-assessing the residual plots and assumptions
par(mfrow=c(2,2))
plot(full.modelwinter)
summary(full.modelwinter)
```
Upon further examination, we find that the following are insignificant and will be fruther analyzesd for removal from the model:  
- `year`
- `infant_deaths`
- `alcohol`
- `hepatitis_b`
- `total_expenditure`
- `status:schooling` interaction


## Splitting the data into test and training sets for cross validation
```{r}
# Cross Validation on Train vs Test 
# Splitting 2939 observations of the data into Training and Test set in the ratio of 70:30
set.seed(1234)
trainIndices = sample(seq(1:dim(Life)[1]),round(.7*dim(Life)[1]))
train = Life[trainIndices,]
test = Life[-trainIndices,]
dim(train)
dim(test)
```


## Creating a prediction function
```{r}
predict.regsubsets = function (object , newdata ,id ,...){
  form=as.formula (object$call [[2]])
  mat=model.matrix(form ,newdata )
  coefi=coef(object ,id=id)
  xvars=names(coefi)
  mat[,xvars]%*%coefi
}
```


## Variable selection - Forward 
```{r, fig.height=7, fig.width=9}
# Just the training set
library(leaps)

reg.fwd=regsubsets(life_expectancy~. + I(hiv_aids^2) + I(sqrt(percentage_expenditure)) + status:percentage_expenditure, data=train, method="forward", nvmax=18)

par(mfrow=c(1,3))

# BIC
bics<-summary(reg.fwd)$bic
plot(1:18,bics,type="l",ylab="BIC",xlab="# of predictors")
index<-which(bics==min(bics))
points(index,bics[index],col="red",pch=10)

# Adjusted R2
adjr2<-summary(reg.fwd)$adjr2
plot(1:18,adjr2,type="l",ylab="Adjusted R-squared",xlab="# of predictors")
index<-which(adjr2==max(adjr2))
points(index,adjr2[index],col="red",pch=10)

# RSS
rss<-summary(reg.fwd)$rss
plot(1:18,rss,type="l",ylab="train RSS",xlab="# of predictors")
index<-which(rss==min(rss))
points(index,rss[index],col="red",pch=10)
mtext("Variable selection - Forward", side = 3, line = -2, outer = TRUE)

# -> 12 predictors
```


## Variable selection - Backward
```{r, fig.height=7, fig.width=9}
# Just the training set
reg.bwd=regsubsets(life_expectancy~. + I(hiv_aids^2) + I(sqrt(percentage_expenditure)) + status:percentage_expenditure, data=train, method="backward", nvmax=18)

par(mfrow=c(1,3))

# BIC
bics2<-summary(reg.bwd)$bic
plot(1:18,bics2,type="l",ylab="BIC",xlab="# of predictors")
index2<-which(bics2==min(bics2))
points(index2,bics2[index2],col="red",pch=10)

# Adjusted R2
adjr2<-summary(reg.bwd)$adjr2
plot(1:18,adjr2,type="l",ylab="Adjusted R-squared",xlab="# of predictors")
index.r2<-which(adjr2==max(adjr2))
points(index.r2,adjr2[index.r2],col="red",pch=10)

# RSS
rss2<-summary(reg.bwd)$rss
plot(1:18,rss2,type="l",ylab="train RSS",xlab="# of predictors")
index.rss2<-which(rss2==min(rss2))
points(index.rss2,rss[index.rss2],col="red",pch=10)
mtext("Variable selection - Backward", side = 3, line = -2, outer = TRUE)

# -> 12 predictors
```


## Variable selection - Stepwise
```{r, fig.height=7, fig.width=9}
# Just the training set
reg.stp=regsubsets(life_expectancy~. + I(hiv_aids^2) + I(sqrt(percentage_expenditure)) + status:percentage_expenditure, data=train, method="seqrep", nvmax=18)

par(mfrow=c(1,3))

# BIC
bics3<-summary(reg.stp)$bic
plot(1:18,bics,type="l",ylab="BIC",xlab="# of predictors")
index3<-which(bics3==min(bics3))
points(index3,bics[index3],col="red",pch=10)

# Adjusted R2
adjr3<-summary(reg.stp)$adjr
plot(1:18,adjr3,type="l",ylab="Adjusted R-squared",xlab="# of predictors")
index3<-which(adjr3==max(adjr3))
points(index3,adjr3[index3],col="red",pch=10)

# RSS
rss3<-summary(reg.stp)$rss
plot(1:18,rss3,type="l",ylab="train RSS",xlab="# of predictors")
index3<-which(rss3==min(rss3))
points(index3,rss[index3],col="red",pch=10)
mtext("Variable selection - Stepwise", side = 3, line = -2, outer = TRUE)

# -> 13 predictors
```


## Variable selection - LASSO
```{r, fig.height=7, fig.width=9}
library(glmnet)


x=model.matrix(life_expectancy~. + I(hiv_aids^2) + I(sqrt(percentage_expenditure)) + status:percentage_expenditure, train)[,-1]
y=train$life_expectancy

xtest<-model.matrix(life_expectancy~. + I(hiv_aids^2) + I(sqrt(percentage_expenditure)) + status:percentage_expenditure, test)[,-1]
ytest<-test$life_expectancy


grid=10^seq(10,-2, length =100)
lasso.mod=glmnet(x,y,alpha=1, lambda =grid)


# LASSO regularization and rsquare plot
plot(lasso.mod,xvar="lambda",label=TRUE)  # shows shrinkage and variable selection
plot(lasso.mod,xvar="dev",label=TRUE)  # shows rsquare change as the number of variable changes

# CV plot for LASSO
cv.out=cv.glmnet(x,y,alpha=1) #alpha=1 performs LASSO
plot(cv.out) #The output has 11 non-zero coefficients which shows that the function has chosen the second vertical second line on the cross-validation plot (within one standard error of the minimum) because cross validation error is measured with some variance.

# Two vertical lines.
# 1. The one is at the minimum(the one at 18)
# 2. The other vertical line is within one standard error of the minimum(the one at 11). We have this line because cross validation error is measured with some variance. 

# => The result chose the second line of 11 variables as seen in the plot(cv.out)

bestlambda<-cv.out$lambda.min  #Optimal penalty parameter.  You can make this call visually.
lasso.pred=predict (lasso.mod ,s=bestlambda ,newx=xtest)

testMSE_LASSO<-mean((ytest-lasso.pred)^2)
testMSE_LASSO


#Coefficients of the lasso model - Shows 11 variables (10 explanatory variables + 1 intercept), here's a coefficient function extractor that works on a cross validation object and pick the coefficient vector corresponding to the best model
coef(lasso.mod,s=bestlambda)   

# Recreating the final LASSO model based on variable selection
# reg.lso <- lm(life_expectancy ~ adult_mortality + polio + diphtheria + hiv_aids + thinness_10_19_years + income_composition_of_resources + schooling + I(sqrt(percentage_expenditure)), data=Life)

reg.lso <- lm(life_expectancy ~ adult_mortality + measles + polio + diphtheria + hiv_aids + thinness_10_19_years + income_composition_of_resources + schooling + I(sqrt(percentage_expenditure)), data=Life)

```


## Cross validating the models on the test data
```{r}
testASEfwd<-c()
testASEbwd<-c()
testASEstp<-c()

for (i in 1:18){
  # predicting the forward model on test set
  predictionsfwd<-predict.regsubsets(object=reg.fwd,newdata=test,id=i)
  testASEfwd[i]<-mean((test$life_expectancy-predictionsfwd)^2)
  
  # predicting the backward model on test set
  predictionsbwd<-predict.regsubsets(object=reg.bwd,newdata=test,id=i)
  testASEbwd[i]<-mean((test$life_expectancy-predictionsbwd)^2)
  
  # predicting stepwise model on test set
  predictionsstp<-predict.regsubsets(object=reg.stp,newdata=test,id=i)
  testASEstp[i]<-mean((test$life_expectancy-predictionsstp)^2)
  
}

```


## Plots for ASE
```{r, fig.height=7, fig.width=9}
par(mfrow=c(1,1))
plot(1:18,testASEfwd,type="l",xlab="# of predictors",ylab="test vs train ASE",ylim=c(10,50))
index<-which(testASEfwd==min(testASEfwd))
points(index,testASEfwd[index],col="red",pch=10)
rss<-summary(reg.fwd)$rss
lines(1:18,rss/2928,lty=3,col="blue")  #Dividing by 2928 since ASE=RSS/sample size
mtext("ASE - Forward", side = 3, line = -2, outer = TRUE)


par(mfrow=c(1,1))
plot(1:18,testASEbwd,type="l",xlab="# of predictors",ylab="test vs train ASE",ylim=c(10,50))
index<-which(testASEbwd==min(testASEbwd))
points(index,testASEbwd[index],col="red",pch=10)
rss<-summary(reg.bwd)$rss
lines(1:18,rss/2928,lty=3,col="blue")  #Dividing by 2928 since ASE=RSS/sample size
mtext("ASE - Backward", side = 3, line = -2, outer = TRUE)


par(mfrow=c(1,1))
plot(1:18,testASEstp,type="l",xlab="# of predictors",ylab="test vs train ASE",ylim=c(10,50))
index<-which(testASEstp==min(testASEstp))
points(index,testASEstp[index],col="red",pch=10)
rss3<-summary(reg.stp)$rss3
lines(1:18,rss/2928,lty=3,col="blue")  #Dividing by 2928 since ASE=RSS/sample size
mtext("ASE - Stepwise", side = 3, line = -2, outer = TRUE)
```



## Using forward backward, and stepwise to select max number of variables
```{r, fig.height=7, fig.width=9}

# the forward and backward models are exactly the same, we will proceed in analyzing only the bakward model


# final variable selection for backward variable selection - the forward and backwards method selected the same model in this case
reg.finalbwd=regsubsets(life_expectancy~. ,data=Life,method="backward",nvmax=12)
coef(reg.finalbwd,10)

final.modelbwd <- lm(life_expectancy ~ status + adult_mortality + percentage_expenditure + measles + polio + diphtheria + hiv_aids + thinness_10_19_years + income_composition_of_resources + schooling + status:percentage_expenditure + I(hiv_aids^2) + I(sqrt(percentage_expenditure)), data=Life)

summary(final.modelbwd)


plot(final.modelbwd$fitted.values,Life$life_expectancy,xlab="Predicted",ylab="Life Expectancy",xlim=c(20,110),ylim=c(20,110))
lines(c(20,110),c(20,110),col="red")
mtext("Backward", side = 3, line = -2, outer = TRUE)




# final variable selection for stepwise variable selection
reg.finalstp=regsubsets(life_expectancy~.,data=Life,method="seqrep",nvmax=13)
coef(reg.finalstp,10)

final.modelstp <- lm(life_expectancy ~ status + adult_mortality + percentage_expenditure + measles + polio + total_expenditure + diphtheria + hiv_aids + thinness_10_19_years + income_composition_of_resources + schooling + status:percentage_expenditure + I(hiv_aids^2) + I(sqrt(percentage_expenditure)), data=Life)

summary(final.modelstp)

plot(final.modelstp$fitted.values,Life$life_expectancy,xlab="Predicted",ylab="Life Expectancy",xlim=c(20,110),ylim=c(20,110) )
lines(c(20,110),c(20,110),col="red")
mtext("Stepwise", side = 3, line = -2, outer = TRUE)


# final variable selection for lasso variable selection
final.modellso <- reg.lso

summary(final.modellso)

plot(final.modellso$fitted.values,Life$life_expectancy,xlab="Predicted",ylab="Life Expectancy",xlim=c(20,110),ylim=c(20,110))
lines(c(20,110),c(20,110),col="red")
mtext("Lasso", side = 3, line = -2, outer = TRUE)
```

## ANOVA lack of fit test between backward model and lasso
```{r}
anova(final.modelbwd, final.modellso)
```
H0:  Backward Model Fit = Lasso Model Fit  
Ha:  Backward Model Fit != Lasso Model Fit  

There is sufficient evidence to suggest that these models do not fit the data the same.  

Reject H0

p-value: < 2.2e-16  


## ANOVA lack of fit test between stepwise and backward model
```{r}
anova(final.modelstp, final.modelbwd)
```
H0:  Stepwise Model Fit = Backward Model Fit  
Ha:  Stepwise Model Fit != Backward Model Fit  

There is not enough evidence to suggest that these models fit the data differently.  

Fail to reject H0

p-value: 0.095

## Using forward backward, and stepwise to select max number of variables
```{r, fig.height=7, fig.width=9}
# final predictive model
final.predictive <- final.modelstp


summary(final.predictive)


plot(final.predictive$fitted.values,Life$life_expectancy,xlab="Predicted",ylab="Life Expectancy",xlim=c(20,110),ylim=c(20,110) )
lines(c(20,110),c(20,110),col="red")
mtext("Stepwise", side = 3, line = -2, outer = TRUE)



# final interpretable model
final.interpretable <- lm(life_expectancy ~ status + adult_mortality + percentage_expenditure + measles + polio + total_expenditure + diphtheria + hiv_aids + thinness_10_19_years + income_composition_of_resources + schooling, data=Life)


summary(final.interpretable)
confint(final.interpretable)

plot(final.interpretable$fitted.values,Life$life_expectancy,xlab="Predicted",ylab="Life Expectancy",xlim=c(20,110),ylim=c(20,110) )
lines(c(20,110),c(20,110),col="red")
mtext("Interpretable", side = 3, line = -2, outer = TRUE)
```



```{r, fig.height=7, fig.width=9}
par(mfrow=c(2,2))
plot(final.interpretable)
```



## ANOVA lack of fit test between final predictive model and interpretable model
```{r}
anova(final.predictive, final.interpretable)
```
H0:  Predictive Model Fit = Interpretable Model Fit  
Ha:  Predictive Model Fit != Interpretable Model Fit  

There is sufficient evidence to suggest that these models do not fit the data the same.  

Reject H0

p-value: < 2.2e-16

## Comparing the models
```{r}

# Function for Root Mean Squared Error
RMSE <- function(error) { sqrt(mean(error^2)) }
ASE <- function(error) { mean(error^2) }

RMSE(final.modelbwd$residuals)
RMSE(final.modelstp$residuals)
RMSE(final.modellso$residuals)
RMSE(final.interpretable$residuals)

ASE(final.modelbwd$residuals)
ASE(final.modelstp$residuals)
ASE(final.modellso$residuals)
ASE(final.interpretable$residuals)

summary(final.modelbwd)$adj.r.squared
summary(final.predictive)$adj.r.squared
summary(final.modellso)$adj.r.squared
summary(final.interpretable)$adj.r.squared


AIC(final.modelbwd, final.predictive, final.modellso, final.interpretable)
BIC(final.modelbwd, final.modelstp, final.modellso, final.interpretable)
```

## Model Summary
We see that isn't enough evidence to suggest that the fit between stepwise and backward is different. We do see that both of them are indicated to be better fitting than the lasso model. Additional evidence suggests that both the stepwise and backward models returned lower diagnostic fit metrics in RMSE, BIC, and AIC, and higher Adjusted R Squared compared to the lasso model.

| Model         | ASE      | RMSE     | Adj R^2   | AIC      | BIC      | DF |
|---------------|----------|----------|-----------|----------|----------|----|
| Lasso         | 17.29395 | 4.1586   | 0.8086831 | 16677.15 | 16742.95 | 11 |
| Interpretable | 17.443   | 4.176481 | 0.8069019 | 16706.28 | 16784.04 | 13 |
| Backward      | 15.41182 | 3.925789 | 0.8292704 | 16347.78 | 16437.51 | 15 |
| Stepwise      | 15.39709 | 3.923912 | 0.8293751 | 16346.98 | 16442.69 | 16 |

We will proceed with the stepwise model as it has a slightly lower RMSE, higher Adjusted R Squared, lower AIC, and an additional degree of freedom; even though we see lower lower BIC from the backward model.


```{r}
###KNN Model1 (categorical)
#changing variable name to what I was using in my code from Neil's
LED = Life

##factorizing status into 0-1
x = LED$status
x = factor(x)
LED$status <- sapply(as.integer(x), switch, "Developing" = 0, "Developed" = 1)

#Data Normalization
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x))) }

LED.df <- as.data.frame(lapply(LED[,-1], normalize)) #normalizes data and removes life expectancy as response variable to be predicted

#splicing into test/train set
set.seed(123)
dat.d <- sample(1:nrow(LED.df), size=nrow(LED.df)*0.7, replace = FALSE) #random selection for 70% of data

train.LED <- LED[dat.d,] # 70% of data
test.LED <- LED[-dat.d,] # 30% of data

#creates separate data frame for 'life expectancy' feature
train.LED_labels <- LED[dat.d,1] #1 for life expectency
test.LED_labels <- LED[-dat.d,1]

library(class)
#NROW(train.LED_labels) # number of observations indicates knn(38) sqrt of 1458

ACC.model = data.frame(matrix(ncol = 1, nrow = 0))

for(i in 1:10){
  knn.model <- knn(train=train.LED, test=test.LED, cl=train.LED_labels, k=i) #gave errors due to non-numeric variables, attempting to remove non-numeric values (country & status)
  
  #accuracy test
  ACC.model[i,] = 100* sum(test.LED_labels == knn.model)/NROW(test.LED_labels)
}

max(ACC.model) #retrieves maximum accuracy obtained
which(ACC.model == max(ACC.model)) #finds the k value that gives the maximum accuracy

#convert knn and test set to dataframes for analysis
library(taRifx)

knn.df <- as.data.frame(knn.model)
knn.df <- remove.factors(knn.df) #removes factor levels to make use of comparitive operations

test.LED_labels.df <- as.data.frame(test.LED_labels)

#find standard deviation for test set to use as range for predictions
test.LED.sd <- sd(test.LED_labels.df$test.LED_labels, na.rm = TRUE)

#finding if predictions is within 2 sd of test set

acc_count = data.frame(matrix(ncol = 1, nrow = 0))

knn.df[1,] <= test.LED_labels.df[1,] + (test.LED.sd/4)

for(i in 1:nrow(knn.df)){
if (knn.df[i,] <= test.LED_labels.df[i,] + (2*test.LED.sd) && knn.df[i,] >= test.LED_labels.df[i,] - (2*test.LED.sd)){
  acc_count[i,] = 1
} else {
  acc_count[i,] = 0
}}

accuracy = 100* sum(acc_count == 1)/NROW(acc_count)
accuracy

```

```{r}
###knn.reg model

LED_outcome <- LED %>% select(life_expectancy)
LED.df <- as.data.frame(lapply(LED[,-1], normalize)) #normalizes data and removes life expectancy as response variable to be predicted

str(LED_outcome)
str(LED.df)


library(FNN)

set.seed(1234) # set the seed to make the partition reproducible

# 75% of the sample size
smp_size <- floor(0.75 * nrow(LED.df))

train_ind <- sample(seq_len(nrow(LED.df)), size = smp_size)

# creating test and training sets that contain all of the predictors
reg_pred_train <- LED.df[train_ind, ]
reg_pred_test <- LED.df[-train_ind, ]

LED_outcome_train <- LED_outcome[train_ind, ]
LED_outcome_test <- LED_outcome[-train_ind, ]

model_error = data.frame(matrix(ncol = 2, nrow = 100))
colnames(model_error) <- c('MSE','MAE')

#cycles through k = 1 to 100 storing all mean error values
for(i in 1:100)
{
  
#knn predictions
reg_results <- knn.reg(reg_pred_train, reg_pred_test, LED_outcome_train, k = i)

#mean square prediction error
model_error[i,1] = mean((LED_outcome_test - reg_results$pred) ^ 2)

#mean absolute error
model_error[i,2] = mean(abs(LED_outcome_test - reg_results$pred))
}

min(model_error$MSE) #retrieves minimum errorobtained
MSE_val <- which(model_error$MSE == min(model_error$MSE)) #finds the k value that gives the min MSE
min(model_error$MAE) #retrieves minimum errorobtained
MAE_val <- which(model_error$MAE == min(model_error$MAE)) #finds the k value that gives the min MAE

model_error[MSE_val,]
model_error[MAE_val,]

#Results of K that produced lowest MSE
reg_results <- knn.reg(reg_pred_train, reg_pred_test, LED_outcome_train, k = 3)

plot(LED_outcome_test~reg_results$pred, cex = .8, col = "dodgerblue", main = "k = 3", xlab="Predicted Values", ylab="Actual Values")
reg_line = lm(LED_outcome_test~reg_results$pred)
abline(reg_line, col = "red")
```
#### Random Forest Model

```{r, fig.width=12, fig.height=6}

# Random Forest Train/Test split
library(MASS)
library(randomForest)
set.seed(128)
trainIndices = sample(1:nrow(Life), nrow(Life)/2)
tree.test = Life[-trainIndices,"life_expectancy"]

# Tuning to find mtry value
tuneRF(Life, life_expectancy)

# Prediction
rf.le = randomForest(life_expectancy~.,data=Life,subset=trainIndices, mtry=5, importance=TRUE, proximity=TRUE)
yhat.rf = predict(rf.le, newdata = Life[-trainIndices,])

# ASE
mean((yhat.rf-tree.test)^2)

# Plot
plot(yhat.rf,tree.test, main="Random Forest Results", xlab="Predicted Values", ylab="Actual Values", col="dodgerblue2")
abline(0,1, col="red")
mean((yhat.rf-tree.test)^2)

# Importance of variables
importance(rf.le)
varImpPlot(rf.le)

# Print
print(rf.le)
summary(Life)

```
```