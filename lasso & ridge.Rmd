---
title: "lasso ridge"
author: "Bo Yun"
date: "10/1/2020"
output: html_document
---

# Variable selection - LASSO
```{r}
library(glmnet)

#Formatting data for GLM net
x=model.matrix(life_expectancy~.,train)[,-1]
y=train$life_expectancy

xtest<-model.matrix(life_expectancy~.,test)[,-1]
ytest<-test$life_expectancy


grid=seq(10,-5, length =100)
lasso.mod=glmnet(x,y,alpha=1)
plot(lasso.mod,xvar="lambda",label=TRUE)
plot(lasso.mod,xvar="dev",label=TRUE)


# CV plot for LASSO
cv.out=cv.glmnet(x,y,alpha=1) #alpha=1 performs LASSO
plot(cv.out)  #The output has 11 non-zero coefficients which shows that the function has chosen the second vertical second line on the cross-validation plot (within one standard error of the minimum) because cross validation error is measured with some variance.

# Two vertical lines.
# 1. The one is at the minimum(the one at 18)
# 2. The other vertical line is within one standard error of the minimum(the one at 11). We have this line because cross validation error is measured with some variance. 

# => The result chose the second line of 11 variables as seen in the plot(cv.out)


bestlambda<-cv.out$lambda.min  #Optimal penalty parameter.  You can make this call visually.
lasso.pred=predict (lasso.mod ,s=bestlambda ,newx=xtest)
testMSE_LASSO<-mean((ytest-lasso.pred)^2)
testMSE_LASSO

coef(lasso.mod,s=bestlambda)    #Coefficients of the lasso model - Shows 11 variables (10 explanatory variables + 1 intercept), here's a coefficient function extractor that works on a cross validation object and pick the coefficient vector corresponding to the best model


# Recreating the final LASSO model based on variable selection
final.model4<-lm(life_expectancy~. -year -hepatitis_b, data=Life)
summary(final.model4)

# Checking assumptions and uncertainty in the prediction
plot(final.model4$fitted.values,Life$life_expectancy,xlab="Predicted",ylab="Life Expectancy",xlim=c(20,110),ylim=c(20,110))
lines(c(20,110),c(20,110),col="red")
```


# Variable selection - RIDGE
```{r}
library(glmnet)

# Formatting data for GLM net
x=model.matrix(life_expectancy~.,train)[,-1]
y=train$life_expectancy

xtest<-model.matrix(life_expectancy~.,test)[,-1]
ytest<-test$life_expectancy

grid=10^seq(10,-2, length =100)
ridge.mod=glmnet(x,y,alpha=0, lambda =grid)

# Ridge plot
cv.ridge=cv.glmnet(x,y,alpha=0) #alpha=0 performs Ridge
plot(cv.ridge)

bestlambda.ridge<-cv.ridge$lambda.min  #Optimal penalty parameter.  You can make this call visually.
ridge.pred=predict (ridge.mod ,s=bestlambda.ridge ,newx=xtest)

# MSE of RIDGE
testMSE_RIDGE<-mean((ytest-ridge.pred)^2)
testMSE_RIDGE

# Ridge coefficient table and variable selection
coef(ridge.mod,s=bestlambda.ridge)  # Utilizing all variables in this model

# Recreating the final RIDGE model based on variable selection
final.model5<-lm(life_expectancy~., data=Life) 
summary(final.model5)

# Checking assumptions and uncertainty in the prediction
plot(final.model5$fitted.values,Life$life_expectancy,xlab="Predicted",ylab="Life Expectancy",xlim=c(20,110),ylim=c(20,110))
lines(c(20,110),c(20,110),col="red")
```